{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "\n",
    "import difflib \n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats.stats import pearsonr  \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter the data\n",
    "# col_to_drop = ['weather','date_time','daily_mape','mape_std']\n",
    "# col_to_drop = []\n",
    "\n",
    "# east_data = east_data[(east_data.weather =='sunny')]\n",
    "# west_data = west_data[(west_data.weather =='sunny')]\n",
    "# lower_data = lower_data[(lower_data.weather =='sunny')]\n",
    "\n",
    "# east_data = east_data[east_data.weather != 'sunny'].drop(col_to_drop,axis=1)\n",
    "# west_data = west_data[west_data.condition == 'normal'].drop(col_to_drop,axis=1)\n",
    "# lower_data = lower_data[lower_data.condition == 'normal'].drop(col_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define dataset\n",
    "# dataset = east_data\n",
    "# daily_mape = process.daily_dict(dataset,'daily_mape')\n",
    "# daily_weather = process.daily_dict(dataset,'weather')\n",
    "# daily_std = process.daily_dict(dataset,'mape_std')\n",
    "# daily_empty = process.daily_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Daily MAPE histogram\n",
    "# visual.histogram(list(east_data['weather'].values),\"east_hist\",False,500)\n",
    "# visual.histogram(list(west_data['daily_mape'].values),\"west_hist\",True,500)\n",
    "# visual.histogram(list(lower_data['daily_mape'].values),\"south_hist\",True,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict power output by regression and reture the predictions for each panel\n",
    "# res1,pred1 = model.model_predict(east_data,True)\n",
    "# res2,pred2 = model.model_predict(west_data,True)\n",
    "# res3,pred3 = model.model_predict(lower_data,True)\n",
    "\n",
    "# # Convert predictions into DataFrame format for each panel\n",
    "# df = pd.DataFrame((pred1[0]))\n",
    "# for i in range(1,len(pred1)):\n",
    "#     df[str(i)] = pd.DataFrame(pred1[i])\n",
    "    \n",
    "# # Save the predictions \n",
    "# export_csv = df.to_csv (\"east_normal_pred.csv\", index = False, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manually calculate MAPE and find distribution\n",
    "# dataset = pd.read_csv('east_normal_gt_pred.csv')\n",
    "\n",
    "# n = len(dataset.columns)//2\n",
    "# errors = {'MAPE':[],'MSE':[],'MAE':[]}\n",
    "\n",
    "# error_high = []\n",
    "\n",
    "# for i in range(n):\n",
    "#     col = 'panel_'+str(i+1)\n",
    "#     gt = dataset[col+'_power']\n",
    "#     pred = dataset[col+'_predict']\n",
    "#     error = []\n",
    "#     high_error_count = 0\n",
    "    \n",
    "#     for j in range(len(gt)):\n",
    "#         err = np.abs(gt[j]-pred[j])/gt[j]\n",
    "#         if err == np.float64('inf') or err == np.float64('nan'):\n",
    "#             continue\n",
    "#         error.append(np.abs(gt[j]-pred[j])/gt[j])\n",
    "        \n",
    "#         if err > 0.05:\n",
    "#             high_error_count += 1\n",
    "#     error_high.append(high_error_count)\n",
    "\n",
    "#     errors['MAPE'].append(np.nansum(error)/len(error)*100)\n",
    "#     errors['MSE'].append(mean_squared_error(gt,pred))\n",
    "#     errors['MAE'].append(mean_absolute_error(gt,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for day in set(wrong_date):\n",
    "#     data = daily_data[day].drop(['weather','daily_mape','mape_std','condition'],axis=1)\n",
    "#     mape = round(df[df['date']==day]['daily_mape'],2)\n",
    "#     std = round(df[df['date']==day]['mape_std'],2)\n",
    "#     corr = round(df[df['date']==day]['correlation'],3)\n",
    "#     weather = df[df['date']==day]['weather']\n",
    "    \n",
    "# #     title = \"MAPE \"+str(mape)+\"; STD \"+str(std)+\"; corr \"+str(corr)+\"; weather \" + weather\n",
    "#     title = \"MAPE \"+str(mape)\n",
    "#     visual.raw_data_plot(data,title,save=False,resolution=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miss_data = ['20190829','20190930','20190905','20190906','20190917','20190919','20191010','20191017',\n",
    "#              '20191031','20191120','20191125','20191213','20190223','20191012','20180516','20180820',\n",
    "#              '20190510','20170920','20190824','20190827','20180829']\n",
    "\n",
    "# for miss in miss_data:\n",
    "#     data = raw_data[miss].drop(['weather','daily_mape','mape_std','condition'],axis=1)\n",
    "#     visual.raw_data_plot(data,miss)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a new dataframe from old one\n",
    "# df = pd.DataFrame()\n",
    "# for day in raw_data:\n",
    "#     data = raw_data[day]\n",
    "#     data[\"date\"] = day\n",
    "#     df = pd.concat([df,data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Weather Pie Chart\n",
    "# weathers = []\n",
    "# for _, i in daily_weather.items():\n",
    "#     weathers.append(i)\n",
    "# visual.pie_chart(weathers,\"Weather Condition - 860 Days\",True,500)\n",
    "\n",
    "\n",
    "\n",
    "# # Bar Chart\n",
    "# fig = plt.figure(figsize=(10,7))\n",
    "# ax = fig.add_subplot(1,1,1)\n",
    "        \n",
    "# bar = plt.bar([i for i in range(1,13)],month_dist)\n",
    "# plt.ylim([0,60])\n",
    "\n",
    "# # # Time x-axis(bottom)\n",
    "# month = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "# ax.set_xticks(month)\n",
    "# ax.set_xticklabels(month, fontsize=25)  \n",
    "\n",
    "# day_space = [0,10,20,30,40,50,60]\n",
    "# ax.set_yticks(day_space)\n",
    "# ax.set_yticklabels(day_space, fontsize=25)  \n",
    "\n",
    "# ax.set_xlabel('Month',fontsize=30)\n",
    "# ax.set_ylabel('Days',fontsize=30)\n",
    "# ax.set_title('Moderate Correlated Data Distribution',fontsize=30,y=1.05)\n",
    "\n",
    "# plt.savefig('moderate_month.jpg', format='jpg', dpi=1000, bbox_inches='tight')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Daily plot\n",
    "\n",
    "# for day in day_list:\n",
    "#     data = dataset[dataset['date']==day].drop(['date'],axis=1)\n",
    "    \n",
    "#     fig = plt.figure(figsize=(10,7))\n",
    "#     ax = fig.add_subplot(1,1,1)\n",
    "    \n",
    "#     box = plt.plot(data)\n",
    "#     plt.legend(['Panel '+str(i+1) for i in range(len(data)-1)])\n",
    "\n",
    "#     title = '-'.join(day.split('/'))\n",
    "#     ax.set_title(title,fontsize=20,y=1.05)\n",
    "    \n",
    "#     plt.savefig(title + '.jpg', format='jpg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate features\n",
    "# date = []\n",
    "# mape = []\n",
    "# std = []\n",
    "# power = []\n",
    "# coeff = []\n",
    "# corr = []\n",
    "\n",
    "# for day in day_list:\n",
    "#     data = dataset[dataset['date']==day]\n",
    "    \n",
    "#     if len(data) < 4:\n",
    "#         continue\n",
    "    \n",
    "#     cor = data['correlation'].values[0]\n",
    "#     p_level = data['power_level'].values[0]\n",
    "    \n",
    "#     if cor == 'strong':\n",
    "#         corr.append(1)\n",
    "#     elif cor == 'moderate':\n",
    "#         corr.append(2)\n",
    "#     else:\n",
    "#         corr.append(3)\n",
    "        \n",
    "#     if p_level == 'high':\n",
    "#         power.append(1)\n",
    "#     else:\n",
    "#         power.append(0)\n",
    "    \n",
    "#     drop_col = [d_col for d_col in data.columns if d_col[:5] != 'panel']\n",
    "#     data = data.drop(drop_col,axis=1)\n",
    "\n",
    "#     res,_ = model.model_predict(data)\n",
    "    \n",
    "#     # Correlation feature\n",
    "#     corr_res = []\n",
    "#     first = 'panel_1_power_28563593'\n",
    "#     for col in data.columns:\n",
    "#         if col != first:\n",
    "#             corr_res.append(pearsonr(data[col].values,data[first].values)[0])\n",
    "#     coeff.append(min(corr_res))\n",
    "    \n",
    "    \n",
    "    \n",
    "#     date.append(day)\n",
    "#     mape.append(np.mean(res))\n",
    "#     std.append(np.std(res))\n",
    "    \n",
    "\n",
    "# feature = pd.DataFrame()\n",
    "# feature['date'] = date\n",
    "# feature['mape'] = mape\n",
    "# feature['std'] = std\n",
    "# feature['power'] = power\n",
    "# feature['coeff'] = coeff\n",
    "# feature['corr'] = corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Panel Number Experiment\n",
    "\n",
    "# dataset = pd.read_csv('lower_data_clean.csv').dropna()\n",
    "# dataset = dataset[dataset['correlation']!='low']\n",
    "\n",
    "# # Create object\n",
    "# visual = helper.visualizer()\n",
    "# model = helper.modeling()\n",
    "# process = helper.data_processing()\n",
    "\n",
    "# drop_col = [d_col for d_col in dataset.columns if d_col[:5] != 'panel']\n",
    "# dataset = dataset.drop(drop_col,axis=1)\n",
    "\n",
    "# num_panel = [2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# target_cols = {i:[] for i in num_panel}\n",
    "# for num in num_panel:\n",
    "#     count = 0\n",
    "    \n",
    "#     while count < 5:\n",
    "#         cols = []\n",
    "#         while len(cols) < num:\n",
    "#             col = random.choice(list(dataset.columns))\n",
    "#             if col not in cols:\n",
    "#                 cols.append(col)\n",
    "#         target_cols[num].append(cols)\n",
    "        \n",
    "#         count += 1\n",
    "\n",
    "# all_res = {i:[] for i in range(len(num_panel))}\n",
    "# test = []\n",
    "\n",
    "# for i in range(len(num_panel)):\n",
    "#     num = num_panel[i]\n",
    "#     cols = target_cols[num]\n",
    "#     for col in cols:\n",
    "#         data = dataset[col]\n",
    "#         res,_ = model.model_predict(data)\n",
    "#         all_res[i].append(np.mean(res))\n",
    "        \n",
    "# results = []\n",
    "\n",
    "# for i in all_res:\n",
    "#     results.append(all_res[i])\n",
    "    \n",
    "\n",
    "# # make the plot\n",
    "# fig = plt.figure(figsize=(8,5))\n",
    "# ax = fig.add_subplot(1,1,1)\n",
    "    \n",
    "# box = plt.boxplot(results,patch_artist=False, showmeans=True)\n",
    "    \n",
    "# ax.xaxis.set_ticklabels([\"1\", \"2\", \"3\",\"4\", \"5\",\n",
    "#                          \"6\", \"7\", \"8\", \"9\"], fontsize=15)\n",
    "# ax.set_xlabel('The Numbers of Panel',fontsize=20)\n",
    "\n",
    "# power_y = [2,4,6,8,10,12,14]\n",
    "# ax.set_yticks(power_y)\n",
    "# ax.yaxis.set_ticklabels(power_y, fontsize=15)\n",
    "# ax.set_ylabel('MAPE(%)',fontsize=20)\n",
    "\n",
    "# # plt.savefig('panel_numbers.jpg', format='jpg', dpi=500, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Geometry experiment\n",
    "# # Load data\n",
    "# east_data = pd.read_csv('east_data_clean.csv').dropna()\n",
    "# east_data = east_data[east_data['correlation']!='low']\n",
    "# west_data = pd.read_csv('west_data_clean.csv').dropna()\n",
    "# west_data = west_data[west_data['correlation']!='low']\n",
    "# lower_data = pd.read_csv('lower_data_clean.csv').dropna()\n",
    "# lower_data = lower_data[lower_data['correlation']!='low']\n",
    "\n",
    "# # Create object\n",
    "# visual = helper.visualizer()\n",
    "# model = helper.modeling()\n",
    "# process = helper.data_processing()\n",
    "\n",
    "# east_drop = [d_col for d_col in east_data.columns if d_col[:5] != 'panel' and d_col not in ['date','time']]\n",
    "# east_data = east_data.drop(east_drop,axis=1)\n",
    "\n",
    "# west_drop = [d_col for d_col in west_data.columns if d_col[:5] != 'panel' and d_col not in ['date','time']]\n",
    "# west_data = west_data.drop(west_drop,axis=1)\n",
    "\n",
    "# lower_drop = [d_col for d_col in lower_data.columns if d_col[:5] != 'panel' and d_col not in ['date','time']]\n",
    "# lower_data = lower_data.drop(lower_drop,axis=1)\n",
    "\n",
    "# date_time = []\n",
    "# indice = east_data.index\n",
    "# for i in indice:\n",
    "#     hour = east_data.loc[i]['time'].split(':')[0]\n",
    "#     minute = east_data.loc[i]['time'].split(':')[1][0]\n",
    "    \n",
    "#     if len(hour) == 1:\n",
    "#         hour = '0'+hour\n",
    "#     date_time.append('-'.join(east_data.loc[i]['date'].split('/'))+'-'+hour+'-'+minute)\n",
    "\n",
    "# east_data['date_time'] = date_time\n",
    "# east_data = east_data.set_index('date_time')\n",
    "\n",
    "# date_time = []\n",
    "# indice = west_data.index\n",
    "# for i in indice:\n",
    "#     hour = west_data.loc[i]['time'].split(':')[0]\n",
    "#     minute = west_data.loc[i]['time'].split(':')[1][0]\n",
    "#     if len(hour) == 1:\n",
    "#         hour = '0'+hour\n",
    "#     date_time.append('-'.join(west_data.loc[i]['date'].split('/'))+'-'+hour+'-'+minute)\n",
    "    \n",
    "# west_data['date_time'] = date_time\n",
    "# west_data = west_data.set_index('date_time')\n",
    "\n",
    "# date_time = []\n",
    "# indice = lower_data.index\n",
    "# for i in indice:\n",
    "#     hour = lower_data.loc[i]['time'].split(':')[0]\n",
    "#     minute = lower_data.loc[i]['time'].split(':')[1][0]\n",
    "#     if len(hour) == 1:\n",
    "#         hour = '0'+hour\n",
    "#     date_time.append('-'.join(lower_data.loc[i]['date'].split('/'))+'-'+hour+'-'+minute)\n",
    "    \n",
    "# lower_data['date_time'] = date_time\n",
    "# lower_data = lower_data.set_index('date_time')\n",
    "\n",
    "# combined_data = lower_data.join(east_data, lsuffix='_caller', rsuffix='_other')\n",
    "# combined_data = combined_data.join(west_data, lsuffix='_caller', rsuffix='_other')\n",
    "# combined_data = combined_data.loc[~combined_data.index.duplicated(keep='first')]\n",
    "# combined_data = combined_data.drop(['date_caller','time_caller','date_other','time_other','date','time'],axis=1)\n",
    "\n",
    "# east_col = ['panel_1_power_28563594','panel_2_power_28563595','panel_3_power_28563596',\n",
    "#             'panel_4_power_28563597','panel_5_power_28563602','panel_6_power_28563603',\n",
    "#             'panel_7_power_28563604']\n",
    "\n",
    "# west_col = ['panel_1_power_28563593','panel_2_power_28563598','panel_3_power_28563600',\n",
    "#             'panel_4_power_28563609','panel_5_power_28563618','panel_6_power_28563619',\n",
    "#             'panel_7_power_28563623']\n",
    "\n",
    "# lower_col = ['panel_1_power_28563601','panel_2_power_28563607','panel_3_power_28563608',\n",
    "#             'panel_4_power_28563610','panel_5_power_28563611','panel_6_power_28563612',\n",
    "#             'panel_7_power_28563613']\n",
    "\n",
    "# all_col = ['panel_1_power_28563594','panel_2_power_28563595','panel_4_power_28563609',\n",
    "#            'panel_5_power_28563618','panel_6_power_28563612','panel_7_power_28563613',\n",
    "#            'panel_2_power_28563598']\n",
    "\n",
    "# cols = [east_col,west_col,lower_col,all_col]\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for col in cols:\n",
    "#     data = combined_data.dropna()[col]\n",
    "#     res,_ = model.model_predict(data)\n",
    "#     results.append(res)\n",
    "    \n",
    "# # make the plot\n",
    "# fig = plt.figure(figsize=(8,5))\n",
    "# ax = fig.add_subplot(1,1,1)\n",
    "    \n",
    "# box = plt.boxplot(results,patch_artist=False, showmeans=True)\n",
    "    \n",
    "# ax.xaxis.set_ticklabels([\"East Roof\", \"West Roof\", \"Lower Roof\",\"Mixed\"], fontsize=15)\n",
    "# # ax.set_xlabel('The Numbers of Panel',fontsize=20)\n",
    "\n",
    "# power_y = [0,1,2,3,4,5,6,7,8]\n",
    "# ax.set_yticks(power_y)\n",
    "# ax.yaxis.set_ticklabels(power_y, fontsize=15)\n",
    "# ax.set_ylabel('MAPE(%)',fontsize=20)\n",
    "\n",
    "# # plt.savefig('direction_experiment.jpg', format='jpg', dpi=500, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Weather experiment 100 days (50 strong - 50 moderate)\n",
    "\n",
    "# def corr_filter(dataset,weather):\n",
    "#     strong = dataset[dataset['weather_icon']==weather][dataset['correlation']=='strong']\n",
    "#     moderate = dataset[dataset['weather_icon']==weather][dataset['correlation']=='moderate']\n",
    "    \n",
    "#     strong_date = list(set(strong['date']))\n",
    "#     moderate_date = list(set(moderate['date']))\n",
    "    \n",
    "    \n",
    "#     if len(strong_date) > 50:\n",
    "#         str_date = []\n",
    "#         while len(str_date) < 50:\n",
    "#             str_date.append(strong_date.pop())\n",
    "#     else:\n",
    "#         str_date = strong_date\n",
    "        \n",
    "#     if len(moderate_date) > 50:\n",
    "#         mod_date = []\n",
    "#         while len(mod_date) < 50:\n",
    "#             mod_date.append(moderate_date.pop())\n",
    "#     else:\n",
    "#         mod_date = moderate_date\n",
    "    \n",
    "#     return str_date, mod_date\n",
    "\n",
    "# def append_date(dataset,str_date,mod_date):\n",
    "    \n",
    "#     model = helper.modeling()\n",
    "    \n",
    "#     df = pd.DataFrame()\n",
    "    \n",
    "#     for i in range(len(str_date)):\n",
    "#         df = df.append(dataset[dataset['date']==str_date[i]])\n",
    "    \n",
    "#     for j in range(len(mod_date)):\n",
    "#         df = df.append(dataset[dataset['date']==mod_date[j]])\n",
    "        \n",
    "#     drop_col = [d_col for d_col in df.columns if d_col[:5] != 'panel']\n",
    "#     df = df.drop(drop_col,axis=1)\n",
    "    \n",
    "#     res,_ = model.model_predict(df,'RF')\n",
    "    \n",
    "#     return res\n",
    "\n",
    "\n",
    "# all_results = []\n",
    "\n",
    "# # Create object\n",
    "# visual = helper.visualizer()\n",
    "# model = helper.modeling()\n",
    "# process = helper.data_processing()\n",
    "\n",
    "# for file in ['east_data_clean.csv']:\n",
    "#     dataset = pd.read_csv(file).dropna()\n",
    "#     dataset = dataset[dataset['correlation']!='low']\n",
    "    \n",
    "#     target_weather = [['clear-day','clear-night'],['cloudy','partly-cloudy-day'],['rain','fog','snow','sleet','wind']]\n",
    "\n",
    "#     col_drop = [d_col for d_col in dataset.columns if d_col[:5] != 'panel' and d_col not in \n",
    "#                 ['weather_icon','correlation','date']]\n",
    "#     dataset = dataset.drop(col_drop,axis=1)\n",
    "    \n",
    "#     n = len(dataset)\n",
    "#     weather_col = []\n",
    "#     indices = [i for i in dataset.index]\n",
    "#     for i in indices:\n",
    "#         weather = dataset.loc[i]['weather_icon']\n",
    "    \n",
    "#         if weather in target_weather[0]:\n",
    "#             weather_col.append('clear')\n",
    "#         elif weather in target_weather[1]:\n",
    "#             weather_col.append('cloudy')\n",
    "#         elif weather in target_weather[2]:\n",
    "#             weather_col.append('rain')\n",
    "#         else:\n",
    "#             weather_col.append('other')\n",
    "\n",
    "#     dataset['weather_icon'] = weather_col\n",
    "    \n",
    "#     rain_str_date, rain_mod_date = corr_filter(dataset,'rain')\n",
    "#     clear_str_date, clear_mod_date = corr_filter(dataset,'clear')\n",
    "#     cloudy_str_date, cloudy_mod_date = corr_filter(dataset,'cloudy')\n",
    "\n",
    "# #     all_results.append(append_date(dataset,rain_str_date,rain_mod_date))\n",
    "#     all_results.append(append_date(dataset,clear_str_date,clear_mod_date))\n",
    "#     all_results.append(append_date(dataset,cloudy_str_date,cloudy_mod_date))\n",
    "    \n",
    "    \n",
    "# # make the plot\n",
    "# fig = plt.figure(figsize=(8,5))\n",
    "# ax = fig.add_subplot(1,1,1)\n",
    "    \n",
    "# box = plt.boxplot(all_results,patch_artist=False, showmeans=True)\n",
    "    \n",
    "# ax.xaxis.set_ticklabels([\"Sunny Day\", \"Cloudy Day\"], fontsize=20)\n",
    "# # ax.set_xlabel('The Numbers of Panel',fontsize=20)\n",
    "\n",
    "# power_y = [0,1,2,3,4,5,6,7,8]\n",
    "# ax.set_yticks(power_y)\n",
    "# ax.yaxis.set_ticklabels(power_y, fontsize=20)\n",
    "# ax.set_ylabel('MAPE(%)',fontsize=25)\n",
    "\n",
    "# # plt.savefig('weather_experiment.jpg', format='jpg', dpi=500, bbox_inches='tight') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Raw data snow defect detection\n",
    "\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics import confusion_matrix \n",
    "\n",
    "# def confusion_plot(cm):\n",
    "#     fig = plt.figure(figsize=(8,8))\n",
    "#     ax = fig.add_subplot(1,1,1) \n",
    "#     sns.set(font_scale=1)\n",
    "#     sns.heatmap(cm, annot=True,fmt='g',annot_kws={\"size\": 20},cmap=\"Blues\", square=True, cbar=False)\n",
    "\n",
    "#     # labels, title and ticks\n",
    "#     ax.set_xlabel('Prediction',fontsize=25)\n",
    "#     ax.set_ylabel('Ground Truth',fontsize=25)\n",
    "#     # title_name = 'Confusion Matrix - Accuracy ' + str(round(acc,2)) + \"%\"\n",
    "#     # ax.set_title(title_name,fontsize=20) \n",
    "#     ax.xaxis.set_ticklabels(['Anomaly', 'Normal'],fontsize=20)\n",
    "#     ax.yaxis.set_ticklabels(['Anomaly', 'Normal'],fontsize=20)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     plt.savefig('anomaly_detection.jpg', format='jpg', dpi=1000)\n",
    "#     plt.show()\n",
    "\n",
    "# dataset = pd.read_csv('lower_data_clean.csv').dropna()\n",
    "# dataset = dataset[dataset['label']!='unknown_issue']\n",
    "\n",
    "# day_list = list(set(dataset['date']))\n",
    "\n",
    "# # Create object\n",
    "# visual = helper.visualizer()\n",
    "# model = helper.modeling()\n",
    "# process = helper.data_processing()\n",
    "\n",
    "# # Generate features\n",
    "# date = []\n",
    "# mape = []\n",
    "# std = []\n",
    "# power = []\n",
    "# coeff = []\n",
    "# corr = []\n",
    "\n",
    "# for day in day_list:\n",
    "#     data = dataset[dataset['date']==day]\n",
    "    \n",
    "#     if len(data) < 4:\n",
    "#         continue\n",
    "    \n",
    "#     cor = data['correlation'].values[0]\n",
    "    \n",
    "#     if cor == 'strong' or cor == 'moderate':\n",
    "#         corr.append(1)\n",
    "#     else:\n",
    "#         corr.append(0)\n",
    "    \n",
    "#     drop_col = [d_col for d_col in data.columns if d_col[:5] != 'panel']\n",
    "#     data = data.drop(drop_col,axis=1)\n",
    "\n",
    "#     res,_ = model.model_predict(data,'RF')\n",
    "    \n",
    "#     # Correlation feature\n",
    "#     corr_res = []\n",
    "#     first = data.columns[0]\n",
    "#     for col in data.columns:\n",
    "#         if col != first:\n",
    "#             corr_res.append(pearsonr(data[col].values,data[first].values)[0])\n",
    "#     coeff.append(min(corr_res))\n",
    "    \n",
    "#     power_level = min([(320-max(data[i]))/320 for i in data.columns])\n",
    "    \n",
    "#     date.append('l'+'/'+day)\n",
    "#     power.append(power_level)\n",
    "#     mape.append(np.mean(res))\n",
    "#     std.append(np.std(res))\n",
    "    \n",
    "\n",
    "# feature = pd.DataFrame()\n",
    "# feature['date'] = date\n",
    "# feature['mape'] = mape\n",
    "# feature['std'] = std\n",
    "# feature['power'] = power\n",
    "# feature['coeff'] = coeff\n",
    "# feature['corr'] = corr\n",
    "\n",
    "# # # export_csv = feature.to_csv (\"lower_features.csv\", index = False, header=True) \n",
    "# # feature = pd.read_csv('sy.csv')\n",
    "# # feature = feature[['date','power','std','coeff','corr']]\n",
    "\n",
    "# df = pd.DataFrame()\n",
    "# df_2 = pd.DataFrame()\n",
    "\n",
    "# df_3 = pd.DataFrame()\n",
    "\n",
    "# thresh = 0.8\n",
    "# df = df.append(feature[feature['corr']==1][feature['power']<thresh])\n",
    "# df = df.append(feature[feature['corr']==0][feature['power']>=thresh])\n",
    "\n",
    "# df_2 = df_2.append(feature[feature['corr']==1][feature['power']>=thresh])\n",
    "# df_2 = df_2.append(feature[feature['corr']==0][feature['power']<thresh])\n",
    "\n",
    "# print(\"df_2 size: {} days\".format(len(df_2)))\n",
    "# df_3 = df_3.append(df)\n",
    "# df_3 = df_3.append(df_2[:20])\n",
    "\n",
    "\n",
    "# print(\"df_3 size: {} days\".format(len(df_3)))\n",
    "# print(\"Undefectable: {} days\".format(len(feature)-len(df_3)))\n",
    "\n",
    "# scores, cm, rf_model,X_test,y_test,test_date = model.random_forest_class(df_3)\n",
    "# scores\n",
    "\n",
    "# # confusion_plot(cm[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model / Method experiment\n",
    "\n",
    "# dataset = pd.read_csv('lower_data_clean.csv').dropna()\n",
    "# dataset = dataset[dataset['correlation']!='low']\n",
    "\n",
    "# col_drop = [d_col for d_col in dataset.columns if d_col[:5] != 'panel']\n",
    "# dataset = dataset.drop(col_drop,axis=1)\n",
    "\n",
    "# # Create object\n",
    "# visual = helper.visualizer()\n",
    "# model = helper.modeling()\n",
    "# process = helper.data_processing()\n",
    "\n",
    "# all_results = []\n",
    "\n",
    "# for method in [\"RF\",\"LR\",\"Naive\"]:\n",
    "#     res,_ = model.model_predict(dataset,method)\n",
    "#     all_results.append(res)\n",
    "    \n",
    "# # make the plot\n",
    "# fig = plt.figure(figsize=(8,5))\n",
    "# ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "# box = plt.boxplot(all_results,patch_artist=False, showmeans=True)\n",
    "    \n",
    "# ax.xaxis.set_ticklabels([\"Random Forest\", \"Linear Regression\", \"Naive Approach\"], fontsize=15)\n",
    "# # ax.set_xlabel('Correlation',fontsize=20)\n",
    "\n",
    "# power_y = [0,3,6,9,12]\n",
    "# ax.set_yticks(power_y)\n",
    "# ax.yaxis.set_ticklabels(power_y, fontsize=15)\n",
    "# ax.set_ylabel('MAPE(%)',fontsize=20)\n",
    "\n",
    "# # plt.savefig('method_experiment.jpg', format='jpg', dpi=500, bbox_inches='tight') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Per panel snow defect detection\n",
    "# def confusion_plot(cm):\n",
    "#     fig = plt.figure(figsize=(8,8))\n",
    "#     ax = fig.add_subplot(1,1,1) \n",
    "#     sns.set(font_scale=1)\n",
    "#     sns.heatmap(cm, annot=True,fmt='g',annot_kws={\"size\": 20},cmap=\"Blues\", square=True, cbar=False)\n",
    "\n",
    "#     # labels, title and ticks\n",
    "#     ax.set_xlabel('Prediction',fontsize=25)\n",
    "#     ax.set_ylabel('Ground Truth',fontsize=25)\n",
    "#     # title_name = 'Confusion Matrix - Accuracy ' + str(round(acc,2)) + \"%\"\n",
    "#     # ax.set_title(title_name,fontsize=20) \n",
    "#     ax.xaxis.set_ticklabels(['Snow Cover', 'Normal','Overcast'],fontsize=20)\n",
    "#     ax.yaxis.set_ticklabels(['Snow Cover', 'Normal','Overcast'],fontsize=20)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     plt.savefig('per_panel_snow_detection.jpg', format='jpg', dpi=1000)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# file = 'lower_data_clean.csv'\n",
    "# dataset = pd.read_csv(file).dropna()\n",
    "# dataset = dataset[dataset['correlation']=='low']\n",
    "\n",
    "# col_drop = [d_col for d_col in dataset.columns if d_col[:5] != 'panel' and d_col \n",
    "#             not in ['date','weather_summary','snow_depth']]\n",
    "# dataset = dataset.drop(col_drop,axis=1)\n",
    "\n",
    "# day_list = list(set(dataset['date']))\n",
    "# panels = [col for col in dataset.columns if col[:5]=='panel']\n",
    "\n",
    "# # Create object\n",
    "# visual = helper.visualizer()\n",
    "# model = helper.modeling()\n",
    "# process = helper.data_processing()\n",
    "\n",
    "# weather_dict = {w:i for w,i in zip(set(dataset['weather_summary']),\n",
    "#                                    [i for i in range(len(set(dataset['weather_summary'])))])}\n",
    "\n",
    "# f_item = []\n",
    "# f_weather = []\n",
    "# f_snow_d = []\n",
    "# f_capacity = []\n",
    "\n",
    "# for day in day_list:\n",
    "\n",
    "#     data = dataset[dataset['date']==day]\n",
    "#     snow_d = data['snow_depth'].values[0]\n",
    "#     weather = collections.Counter(data['weather_summary']).most_common()[0][0]\n",
    "#     weather = weather_dict[weather]\n",
    "#     data = data.drop(['date','snow_depth','weather_summary'],axis=1)\n",
    "    \n",
    "#     for panel in panels:\n",
    "#         max_capacity = max(data[panel].values)/320\n",
    "        \n",
    "#         f_item.append('-'.join(day.split('/')) + '_' +panel[:7])\n",
    "#         f_weather.append(weather)\n",
    "#         f_snow_d.append(snow_d)\n",
    "#         f_capacity.append(max_capacity)\n",
    "\n",
    "# feature = pd.DataFrame()\n",
    "# feature['date'] = f_item\n",
    "# feature['weather'] = f_weather\n",
    "# feature['snow_depth'] = f_snow_d\n",
    "# feature['max_capacity'] = f_capacity\n",
    "\n",
    "# labels = []\n",
    "\n",
    "# n = len(feature)\n",
    "\n",
    "# for i in range(n):\n",
    "#     data = feature.loc[i]\n",
    "#     capacity = data['max_capacity']\n",
    "#     if data['snow_depth'] != 0:\n",
    "#         if capacity < 0.5:\n",
    "#             # snow cover - 0\n",
    "#             labels.append(0)\n",
    "#         else:\n",
    "#             # normal - 1\n",
    "#             labels.append(1)    \n",
    "#     else:\n",
    "#         if capacity < 0.7:\n",
    "#             # overcast - 2\n",
    "#             labels.append(2)\n",
    "#         else:\n",
    "#             # normal - 1\n",
    "#             labels.append(1)\n",
    "            \n",
    "            \n",
    "# feature['labels'] = labels\n",
    "\n",
    "# # export_csv = feature.to_csv (\"lower_feature.csv\", index = False, header=True) \n",
    "\n",
    "# scores, cm, rf_model,X_test,y_test,test_date = model.random_forest_class(feature)\n",
    "# confusion_plot(cm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data Size Experiment \n",
    "# def make_plot(results,save=False):\n",
    "#     # make the plot\n",
    "#     fig = plt.figure(figsize=(8,5))\n",
    "#     ax = fig.add_subplot(1,1,1)\n",
    "    \n",
    "#     title_name = \"Power Prediction Error - East Roof Dataset\"\n",
    "#     box = plt.boxplot(results,patch_artist=False, showmeans=True)\n",
    "    \n",
    "#     ax.xaxis.set_ticklabels([\"1 Day\", \"2 Days\", \"3 Days\", '4 Days', '5 Days'], fontsize=20)\n",
    "#     power_y = [0,2,4,6,8,10,12,14]\n",
    "#     ax.set_yticks(power_y)\n",
    "#     ax.yaxis.set_ticklabels(power_y, fontsize=20)\n",
    "#     ax.set_ylabel('Power(W)',fontsize=25)\n",
    "    \n",
    "#     if save == True:\n",
    "#         plt.savefig('size_experiment.jpg', format='jpg', dpi=500, bbox_inches='tight')\n",
    "        \n",
    "        \n",
    "# file = 'lower_data_clean.csv'\n",
    "# dataset = pd.read_csv(file)\n",
    "# dataset = dataset[dataset['correlation'] == 'moderate']\n",
    "\n",
    "# col_drop = [d_col for d_col in dataset.columns if d_col[:5] != 'panel' and d_col \n",
    "#             not in ['date']]\n",
    "# dataset = dataset.drop(col_drop,axis=1)\n",
    "\n",
    "# day_list = list(set(dataset['date']))\n",
    "# panels = [col for col in dataset.columns if col[:5]=='panel']\n",
    "\n",
    "# # Create object\n",
    "# visual = helper.visualizer()\n",
    "# model = helper.modeling()\n",
    "# process = helper.data_processing()\n",
    "\n",
    "# day_list = []\n",
    "# for day in dataset['date'].values:\n",
    "#     if day not in day_list:\n",
    "#         day_list.append(day)\n",
    "        \n",
    "# one_week = day_list[:1]\n",
    "# one_month = day_list[:2]\n",
    "# half_year = day_list[:3]\n",
    "# one_year = day_list[:4]\n",
    "# two_year = day_list[:6]\n",
    "\n",
    "# time_ = [one_week, one_month, half_year, one_year, two_year]\n",
    "# indices = [i for i in dataset.index]\n",
    "\n",
    "# all_results = []\n",
    "\n",
    "# for days in time_:\n",
    "    \n",
    "#     df = pd.DataFrame()\n",
    "    \n",
    "#     for day in days:\n",
    "    \n",
    "#         df = df.append(dataset[dataset['date']==day].drop(['date'],axis=1))\n",
    "            \n",
    "#     res,_ = model.model_predict(df,'RF')\n",
    "#     all_results.append(res)\n",
    "    \n",
    "# make_plot(all_results, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specificity and sensitivity \n",
    "# TN = (116)\n",
    "# TP = 95+102\n",
    "# FN = 1\n",
    "# FP = 4\n",
    "\n",
    "# spec = (TN)/(TN+FP)\n",
    "# sensi = TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
