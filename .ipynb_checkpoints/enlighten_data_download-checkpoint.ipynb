{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load module and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from datetime import datetime, timedelta\n",
    "from random import randrange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import calendar\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download login info as cookie (run this only when you need cookie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Open the virtual browser \n",
    "# driver = webdriver.Firefox()\n",
    "\n",
    "# ## Open the website \n",
    "# website = 'https://enlighten.enphaseenergy.com/systems/1302574/inverters/28563594/time_series_x?&date=2019-05-04&stat=POWR%2CDCV%2CDCA%2CACV%2CACHZ%2CTMPI'\n",
    "# driver.get(website)\n",
    "\n",
    "# ## Username and password\n",
    "# user=\"mfeng@umass.edu\"\n",
    "# password=\"UmassCS!\"\n",
    "\n",
    "# ## Login the website\n",
    "# driver.find_element_by_id('user_email').click()\n",
    "# driver.find_element_by_id(\"user_email\").send_keys(user)\n",
    "# driver.find_element_by_id('user_password').click()\n",
    "# driver.find_element_by_id(\"user_password\").send_keys(password)\n",
    "# driver.find_element_by_id('submit').click()\n",
    "\n",
    "# ## Store the login info in cookie\n",
    "# driver.get(website)\n",
    "# cookie_items = driver.get_cookies()\n",
    "\n",
    "# post = {}\n",
    "\n",
    "# for cookie_item in cookie_items:\n",
    "#     post[cookie_item['name']] = cookie_item['value']\n",
    "    \n",
    "# cookie_str = json.dumps(post)\n",
    "# with open('cookie.txt', 'w', encoding='utf-8') as f:\n",
    "#     f.write(cookie_str)\n",
    "# f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data for each panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the stored cookie to login and keep session open\n",
    "with open('cookie.txt', 'r',encoding='utf-8') as f:\n",
    "    cookie = f.read()\n",
    "cookies = json.loads(cookie)\n",
    "\n",
    "## Align microinverter's serial number with inverter ID\n",
    "system_id = '1302574'\n",
    "\n",
    "## Inverter serial number and ID is aligned one by one \n",
    "inverter_sn = ['121721038143', '121721037801', '121721037691', '121721038936', '121721038148', \n",
    "             '121721037892', '121721037821', '121721037806', '121721038079', '121721038122', \n",
    "             '121721038133', '121721037662', '121721037689', '121721037871', '121721037817', \n",
    "             '121721038020', '121721038147', '121721038076', '121721038911', '121721037842', \n",
    "             '121721037788', '121721037686', '121721037867', '121721038108', '121721038125', \n",
    "             '121721037685', '121721038107', '121721038154', '121721038144', '121721037880', \n",
    "             '121721038037']\n",
    "\n",
    "inverter_id = [str(28563593+i) for i in range(len(inverter_sn))]\n",
    "inverter_sn_id = {k:v for k,v in zip(inverter_id,inverter_sn)}\n",
    "\n",
    "# print(\"Microinverter ID: serial number\")\n",
    "# print(inverter_sn_id)\n",
    "\n",
    "## Group panels based on their facing direction (west/east/lower)\n",
    "west_roof = ['121721038107', '121721037685', '121721038037', '121721038147', '121721037806', \n",
    "             '121721037892', '121721038143']\n",
    "east_roof = ['121721037871', '121721037662', '121721037801', '121721037691', '121721037880',\n",
    "             '121721038936', '121721038122', '121721038148', '121721038133']\n",
    "lower_roof = ['121721037686', '121721037788', '121721038076', '121721038079', '121721037867', \n",
    "              '121721038020', '121721037817', '121721038144', '121721038911', '121721037842']\n",
    "other_roof = ['121721038125','121721037689','121721037821','121721038154','121721038108']\n",
    "\n",
    "\n",
    "combine_roof = ['121721038107','121721037685','121721038037','121721037871','121721037662','121721038148',\n",
    "            '121721038076', '121721037817', '121721037867']\n",
    "\n",
    "west_roof_id = []\n",
    "east_roof_id = []\n",
    "lower_roof_id = []\n",
    "other_roof_id = []\n",
    "combine_roof_id = []\n",
    "\n",
    "for inverter in inverter_sn:\n",
    "    if inverter in west_roof:\n",
    "        west_roof_id.append(inverter_id[inverter_sn.index(inverter)])\n",
    "    elif inverter in east_roof:\n",
    "        east_roof_id.append(inverter_id[inverter_sn.index(inverter)])\n",
    "    elif inverter in lower_roof:\n",
    "        lower_roof_id.append(inverter_id[inverter_sn.index(inverter)])\n",
    "    elif inverter in other_roof:\n",
    "        other_roof_id.append(inverter_id[inverter_sn.index(inverter)])\n",
    "    \n",
    "# for inverter in inverter_sn:\n",
    "#     if inverter in combine_roof:\n",
    "#         combine_roof_id.append(inverter_id[inverter_sn.index(inverter)])\n",
    "\n",
    "## Formula: website = link_1 + system_id + link_2 + inverter_id + link_3 + date + link_4\n",
    "link_1 = 'https://enlighten.enphaseenergy.com/systems/'\n",
    "link_2 = '/inverters/'\n",
    "link_3 = '/time_series_x?&date='\n",
    "link_4 = '&stat=POWR%2CDCV%2CDCA%2CACV%2CACHZ%2CTMPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calendar_test(day, count, date_list = None):\n",
    "    if date_list is None:\n",
    "        date_list = []\n",
    "    if count == 0:\n",
    "        return date_list\n",
    "    count -= 1\n",
    "    day = day + timedelta(days=1)\n",
    "    date = day.isoformat().split('T')[0]\n",
    "    date_list.append(date)\n",
    "    \n",
    "    calendar_test(day,count,date_list)\n",
    "    \n",
    "    return date_list\n",
    "\n",
    "def download_data(start_day, end_day, roof_panel, save=False):\n",
    "    \n",
    "    start_day = datetime.strptime(start_day, '%Y-%m-%d')\n",
    "    end_day = datetime.strptime(end_day, '%Y-%m-%d')\n",
    "    total_day = abs((end_day - start_day).days)\n",
    "    day_list = calendar_test(start_day,total_day)\n",
    "    \n",
    "    for inverter_id in roof_panel:\n",
    "        print('Inverter ID: {}'.format(inverter_id))\n",
    "    \n",
    "        info = {'date_time':[],'epoch':[], 'power':[], 'power_unknown':[], 'DCV':[], \n",
    "                'DCA':[], 'ACV':[], 'ACHZ':[], 'TMPI':[]}\n",
    "        for day in day_list:\n",
    "            # website address where data is stored\n",
    "            website = link_1 + system_id + link_2 + inverter_id + link_3 + day + link_4\n",
    "            # access data in json format from the website\n",
    "            res = requests.get(url=website, cookies=cookies)\n",
    "            data = res.json()\n",
    "        \n",
    "            # extract time information \n",
    "            for ii in data['POWR']:\n",
    "                epoch = ii[0]\n",
    "                info['epoch'].append(epoch)\n",
    "                \n",
    "                date_time = time.struct_time(time.localtime(ii[0]))\n",
    "                date = 10000*date_time[0]+100*date_time[1]+1*date_time[2]\n",
    "                time_ = 100*date_time[3]+1*date_time[4]\n",
    "                date_time = date*10000+time_                \n",
    "            \n",
    "                info['date_time'].append(date_time)\n",
    "                info['power'].append(ii[1])\n",
    "                info['power_unknown'].append(ii[2])\n",
    "                        \n",
    "            for ii in data['DCV']:\n",
    "                info['DCV'].append(ii[1])\n",
    "            \n",
    "            for ii in data['DCA']:\n",
    "                info['DCA'].append(ii[1])\n",
    "            \n",
    "            for ii in data['ACV']:\n",
    "                info['ACV'].append(ii[1])\n",
    "            \n",
    "            for ii in data['ACHZ']:\n",
    "                info['ACHZ'].append(ii[1])\n",
    "            \n",
    "            for ii in data['TMPI']:\n",
    "                info['TMPI'].append(ii[1])\n",
    "            \n",
    "        # Save downloaded data for each panel\n",
    "        data = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in info.items() ]))\n",
    "        if save is True: \n",
    "            export_csv = data.to_csv (str(inverter_id)+\".csv\", index = True, header=True)\n",
    "            \n",
    "    return data\n",
    "    \n",
    "def daily_data(start_day, end_day, roof_panel, save=False):\n",
    "    \n",
    "    start_day = datetime.strptime(start_day, '%Y-%m-%d')\n",
    "    end_day = datetime.strptime(end_day, '%Y-%m-%d')\n",
    "    total_day = abs((end_day - start_day).days)\n",
    "    day_list = calendar_test(start_day,total_day)\n",
    "    \n",
    "    for inverter_id in roof_panel:\n",
    "        print('Inverter ID: {}'.format(inverter_id))\n",
    "        \n",
    "        info = {day:[] for day in day_list}\n",
    "        \n",
    "        for day in day_list:\n",
    "            # website address where data is stored\n",
    "            website = link_1 + system_id + link_2 + inverter_id + link_3 + day + link_4\n",
    "            # access data in json format from the website\n",
    "            res = requests.get(url=website, cookies=cookies)\n",
    "            data = res.json()\n",
    "    \n",
    "            # extract time information \n",
    "            for ii in data['POWR']:\n",
    "                \n",
    "                info[day].append(ii[1])\n",
    "            \n",
    "        # Save downloaded data for each panel\n",
    "        data = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in info.items() ]))\n",
    "\n",
    "        if save is True: \n",
    "            export_csv = data.to_csv (str(inverter_id)+\".csv\", index = True, header=True)\n",
    "            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverter ID: 28563599\n",
      "Inverter ID: 28563605\n",
      "Inverter ID: 28563616\n",
      "Inverter ID: 28563617\n",
      "Inverter ID: 28563620\n"
     ]
    }
   ],
   "source": [
    "start_day = '2017-9-12'\n",
    "end_day = '2020-2-11'\n",
    "roof_panel = other_roof_id \n",
    "data = download_data(start_day, end_day,roof_panel, True)\n",
    "# data = daily_data(start_day, end_day,roof_panel, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):   \n",
    "    ## Load dataset \n",
    "    dataset = pd.read_csv(file, index_col=0)\n",
    "    dataset = dataset.set_index('date_time')\n",
    "    dataset = dataset.drop(['power_unknown', 'DCV', 'DCA', 'ACV', 'ACHZ', 'TMPI'], axis=1)\n",
    "    dataset = dataset.rename(columns = {'power': str(file[:-4])})\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def convert_to_epoch(day_time):\n",
    "    d = str(day_time)[:4]+'-'+str(day_time)[4:6]+'-'+str(day_time)[6:8]+\\\n",
    "    ' '+str(day_time)[8:10]+':'+str(day_time)[10:]\n",
    "    p = '%Y-%m-%d %H:%M'\n",
    "    epoch = int(time.mktime(time.strptime(d,p)))\n",
    "    return epoch\n",
    "\n",
    "def create_daily_data(dataset):\n",
    "    # Create time frame from 6:00AM to 7:00PM\n",
    "    init = 600\n",
    "    minute = 0\n",
    "    time_frame = ['0'+str(init)]\n",
    "    while init <= 1900:\n",
    "        minute += 1\n",
    "        if minute == 60:\n",
    "            minute = 0\n",
    "            init += 100-59\n",
    "        else:\n",
    "            init += 1\n",
    "    \n",
    "        value = str(init)\n",
    "        if len(value) == 3:\n",
    "            value = '0'+value\n",
    "        time_frame.append(value)\n",
    "    \n",
    "    n = len(dataset)\n",
    "    daily_data = {}\n",
    "\n",
    "    for i in range(n):\n",
    "        panel_data = dataset[i]\n",
    "        row_index = panel_data.index\n",
    "        id_ = panel_data.columns[1]\n",
    "    \n",
    "        for index in row_index:\n",
    "            day = str(index)[:8]\n",
    "        \n",
    "            if day not in daily_data:\n",
    "                day_index = [int(day + i) for i in time_frame]\n",
    "                daily_data[day] = pd.DataFrame(index=day_index)\n",
    "            \n",
    "            value = panel_data.loc[index][id_]\n",
    "            if type(value) == np.float64:\n",
    "                daily_data[day].at[index,id_] = panel_data.loc[index][id_]\n",
    "            else:\n",
    "                daily_data[day].at[index,id_] = np.mean(panel_data.loc[index][id_].values) \n",
    "                \n",
    "    return daily_data\n",
    "\n",
    "def combine_data(daily_data,day_list):\n",
    "    \n",
    "    # Columns (panels' ID)\n",
    "    cols = [col for col in daily_data[day_list[0]].columns]\n",
    "\n",
    "    # Create an empty DataFrame with columns of panels' ID\n",
    "    clean = pd.DataFrame(columns=cols)\n",
    "\n",
    "    # Rows (time frame index, from 6am to 7pm with granularity of 1 minute)\n",
    "    rows = [index for index in daily_data[day_list[0]].index]\n",
    "\n",
    "    day_col = []\n",
    "    time_col = []\n",
    "    epoch = []\n",
    "    product_issue = []\n",
    "\n",
    "    for day in day_list:\n",
    "        # Daily data\n",
    "        data = daily_data[day]\n",
    "    \n",
    "        if len(data.columns) != len(cols):\n",
    "            product_issue.append(day)\n",
    "            continue\n",
    "        # Rows index\n",
    "        rows = [index for index in data.index]\n",
    "        for index in rows:\n",
    "            row_values = data.loc[index]\n",
    "    \n",
    "            if row_values.isna().sum() == 0:\n",
    "                clean.at[index] = row_values\n",
    "            \n",
    "                day_col.append(day[0:4]+'-'+day[4:6]+'-'+day[6:])\n",
    "                time_col.append(str(index)[8:10]+':'+str(index)[10:])\n",
    "                epoch.append(convert_to_epoch(index))\n",
    "\n",
    "            elif row_values.isna().sum() == len(cols):\n",
    "                continue\n",
    "            elif row_values.isna().sum() <= 3:\n",
    "                row_values = row_values.values\n",
    "                index_loc = rows.index(index)\n",
    "            \n",
    "                day_col.append(day[0:4]+'-'+day[4:6]+'-'+day[6:])\n",
    "                time_col.append(str(index)[8:10]+':'+str(index)[10:])\n",
    "                epoch.append(convert_to_epoch(index))\n",
    "    \n",
    "                for i in range(len(row_values)):\n",
    "                    if np.isnan(row_values[i]):\n",
    "                        window = [-2,-1,0,1,2]\n",
    "                        nearby_value = []\n",
    "                    \n",
    "                        for loc in window:\n",
    "                            if (index_loc - loc) >= 0 and (index_loc + loc) < len(rows):\n",
    "                                nearby_value.append(data.loc[rows[index_loc+loc]][i])\n",
    "                        nearby_value = [nearby for nearby in nearby_value if ~np.isnan(nearby)]\n",
    "                    \n",
    "                        if len(nearby_value) == 0:\n",
    "                            row_values[i] = 0\n",
    "                        else:\n",
    "                            row_values[i] = np.mean(nearby_value)    \n",
    "                clean.at[index] = row_values\n",
    "        \n",
    "    clean['date'] = day_col\n",
    "    clean['time'] = time_col\n",
    "    clean['epoch_time'] = epoch\n",
    "    \n",
    "    return clean,product_issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aslanfeng/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load files from current directory\n",
    "files = os.listdir(os.getcwd())\n",
    "files = [file for file in files if file[:-4] in roof_panel]\n",
    "\n",
    "# Load data\n",
    "dataset = [load_data(file) for file in files]\n",
    "\n",
    "# Create daily data\n",
    "daily_data = create_daily_data(dataset)\n",
    "\n",
    "# Create day list to access daily data\n",
    "day_list = [i for i in daily_data]\n",
    "\n",
    "# Combine the power of all panels\n",
    "clean_data, issue_day = combine_data(daily_data,day_list)\n",
    "\n",
    "# Add the product issue data\n",
    "for i in issue_day:\n",
    "    anomaly,_ = combine_data(daily_data,[i])\n",
    "    clean_data = pd.concat([clean_data,anomaly], axis=0, ignore_index=True)\n",
    "\n",
    "# Replace NaN data with 0\n",
    "clean_data = clean_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborate with weather and temperature data from DarkSky API\n",
    "# Weather data required\n",
    "\n",
    "def combine_weather_temp(dataset):\n",
    "    \n",
    "    weather = pd.read_csv('darksky_weather_data.csv')\n",
    "    day_list = [day for day in set(dataset['date'])]\n",
    "    \n",
    "    weather_dict = {}\n",
    "\n",
    "    for i in range(len(weather)):\n",
    "\n",
    "        epoch = weather.loc[i]['time']\n",
    "        date_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(epoch))\n",
    "        date = date_time.split(' ')[0]\n",
    "        hour = date_time.split(' ')[1].split(':')[0]\n",
    "    \n",
    "        if date not in weather_dict:\n",
    "            weather_dict[date] = {}\n",
    "        \n",
    "        if hour not in weather_dict[date]:\n",
    "            weather_dict[date][hour] = []\n",
    "            weather_dict[date][hour].append(weather.loc[i]['icon'])\n",
    "            weather_dict[date][hour].append(weather.loc[i]['temperature'])\n",
    "        \n",
    "    weather_col = []\n",
    "    temperature_col = []\n",
    "    n = len(dataset)\n",
    "\n",
    "    for i in range(n):\n",
    "        date = dataset.loc[i]['date']\n",
    "        hour = dataset.loc[i]['time'].split(':')[0]\n",
    "    \n",
    "        if date in weather_dict:\n",
    "            if hour in weather_dict[date]:\n",
    "                weather_col.append(weather_dict[date][hour][0])\n",
    "                temperature_col.append(weather_dict[date][hour][1])\n",
    "            else:\n",
    "                weather_col.append(np.float64('nan'))\n",
    "                temperature_col.append(np.float64('nan'))\n",
    "        else:\n",
    "            weather_col.append(np.float64('nan'))\n",
    "            temperature_col.append(np.float64('nan'))\n",
    "            \n",
    "    for i in range(len(weather_col)):\n",
    "        weather = weather_col[i]\n",
    "        if weather == 'clear-day' or weather == 'clear-night':\n",
    "            weather_col[i] = 'sunny'\n",
    "        elif weather == 'partly-cloudy-day' or weather == 'partly-cloudy-night':\n",
    "            weather_col[i] = 'partly-cloudy'\n",
    "        \n",
    "    dataset['weather']=weather_col\n",
    "    dataset['temperature']=temperature_col\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "clean_data = combine_weather_temp(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \n",
    "# Save clean data\n",
    "export_csv = clean_data.to_csv (title + \".csv\", index = False, header=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
